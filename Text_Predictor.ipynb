{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**POETRY PREDICTOR**\n"
      ],
      "metadata": {
        "id": "Z8ffoBW4pv0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using encoder-decoder architecture. Before writing the functions for either we need to preprocess the dataset.\n",
        "\n",
        "**1. IMPORT DATASET**\n",
        "\n",
        "Import the dataset from a URL using keras api from tensorflow library."
      ],
      "metadata": {
        "id": "pu2N5cAAp3iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRiOhOQ-nV-C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOW TO CHOOSE LOG LEVEL:**\n",
        "\n",
        "**0:** Show all logs, including DEBUG messages.\n",
        "\n",
        "**1:** Filter out DEBUG messages, showing INFO, WARNING, and ERROR messages.\n",
        "\n",
        "**2:** Filter out DEBUG and INFO messages, showing only WARNING and ERROR messages.\n",
        "\n",
        "**3:** Filter out DEBUG, INFO, and WARNING messages, showing only ERROR messages."
      ],
      "metadata": {
        "id": "dUqDwMCtqty3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "dataset_file= tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
      ],
      "metadata": {
        "id": "ytcp58mXrgdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. READ THE FILE**\n",
        "\n",
        "Read the file in binary mode and then decode it to UTF encoded string (Now we have 1 giant string). Now, calculate length of string in terms of characters.\n"
      ],
      "metadata": {
        "id": "01NyvoElrMCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_string= open(dataset_file, 'rb').read().decode('UTF-8')\n",
        "print(extracted_string)"
      ],
      "metadata": {
        "id": "3cKXp6hfrY7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. UNIQUE CHARACTERS**\n",
        "\n",
        "Find out how many unique characters are in the document and store in a variable ‘unique’. This is our **vocabulary**."
      ],
      "metadata": {
        "id": "QxdPy6ydsinF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique= sorted(set(extracted_string))\n",
        "print(unique)"
      ],
      "metadata": {
        "id": "ipqdmI3Fs21O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. GIVE IDS TO CHARACTERS**\n",
        "\n",
        "We have no use for simple characters, so we need to assign some identity to these characters."
      ],
      "metadata": {
        "id": "phcWYfYVtZCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_to_chars= tf.keras.layers.StringLookup(vocabulary= list(unique), mask_token=None)"
      ],
      "metadata": {
        "id": "diazEdn9tpUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a text-generator, we need to be able to convert these Ids back to human-readable words. So we have to write a code to convert from IDs to characters."
      ],
      "metadata": {
        "id": "H6ulkqlbucKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids= tf.keras.layers.StringLookup(vocabulary= ids_to_chars.get_vocabulary(), invert= True, mask_token= None)"
      ],
      "metadata": {
        "id": "gOrcl_6euqhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now perform these steps on the actual extracted data and not just unique characters i.e: **Give ids to all characters in dataset**."
      ],
      "metadata": {
        "id": "KpG4SRcrvY7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids_to_data= ids_to_chars(tf.strings.unicode_split(extracted_string, \"UTF-8\"))"
      ],
      "metadata": {
        "id": "GywN0d1gvfx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a stream of these character indices** obtained from actual extracted data."
      ],
      "metadata": {
        "id": "WfMVi9OHx__O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_indices_stream=tf.data.Dataset.from_tensor_slices(all_ids_to_data)"
      ],
      "metadata": {
        "id": "A3tIFhqOxoMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now perform the inverse."
      ],
      "metadata": {
        "id": "5HFZ7NFZyKUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readable_text_from_indices(ids):\n",
        "   return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "FkoeoWh-y7Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this inversion method we can obtain characters from the ids. However, we need to regenerate our String (sequence) from these characters since our encoder will take an input sequence. To do that we use batch method."
      ],
      "metadata": {
        "id": "OLNAmRMdzxTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#suppose we have sequence length as 100\n",
        "seq_length= 100\n",
        "\n",
        "#from char indice stream, generate a sequence\n",
        "sequences= char_indices_stream.batch(seq_length+1, drop_remainder= True) #take last as empty\n",
        "for seq in sequences.take(1):\n",
        "   print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "2qaUxXVf0Mpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above sequence is just a sequence of characters obtained from indices. Now, we convert these characters into readable text."
      ],
      "metadata": {
        "id": "ciFqk4Xj2TcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(1):\n",
        "   print(readable_text_from_indices(seq).numpy())"
      ],
      "metadata": {
        "id": "-4tkboNy16qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. LABEL AND TARGET**\n",
        "\n",
        "In a predictor, we need a label which is current character and target which is next characters. To do this input is all elements in sequence except last. And label/target is all elements except first."
      ],
      "metadata": {
        "id": "jP1GVMCm2pzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_sequence(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text =sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "1Fu538Y92-mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= sequences.map(split_input_sequence)\n",
        "for input, target in dataset.take(1):\n",
        "   print(\"Input :\", readable_text_from_indices(input).numpy())\n",
        "   print(\"Target:\", readable_text_from_indices(target).numpy())"
      ],
      "metadata": {
        "id": "9PQMiESE3mns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. CREATE A BATCH**"
      ],
      "metadata": {
        "id": "FXQxtqDa4QSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "UzFauJwL4Sof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. BUILDING THE ENCODER**\n",
        "\n",
        "•\tStart token should be vector (a sequence), created using embedding layer.\n",
        "\n",
        "•\tRecurrent layer, updates state produced by encoder to new state, using GRU.\n",
        "\n",
        "•\tPass new state to dense layer (softmax layer) to produce probability.\n"
      ],
      "metadata": {
        "id": "3qudxAel8IBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size= len(unique)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "rwYzm4w7DLmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder inherits from **tf.keras.Model**, which is a base class for building models in TensorFlow. This means it will have all the methods and properties of tf.keras.Model.\n",
        "\n",
        "**self** is a conventional name used to represent the instance of the class within its own methods."
      ],
      "metadata": {
        "id": "0VrILL3PEbfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self) #inheritance from parent\n",
        "    self.embedding= tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru =tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True) #gives sequence and hidden states\n",
        "    self.dense= tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    #The call method handles the forward pass of the model, applying each layer to the inputs.\n",
        "    #If no previous state is provided, it initializes the state; otherwise, it uses the given state.\n",
        "    #The method can also return the updated state if requested, which is useful for generating text iteratively.\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "       x= self.embedding(inputs, training=training)\n",
        "       if states is None:\n",
        "         states= self.gru.get_initial_state(x)     #if no prev. state exists, use current state.\n",
        "  # if a prev. state exists\n",
        "         x, states = self.gru(x, initial_state= states, training=training)\n",
        "         x= self.dense(x, training=training)\n",
        "\n",
        "         if return_state:\n",
        "             return x, states\n",
        "         else:\n",
        "          return x\n"
      ],
      "metadata": {
        "id": "HrWOLzhjDoEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder_model= Encoder(vocab_size= len(ids_to_chars.get_vocabulary()), embedding_dim=embedding_dim, rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "yfqD2r3KKTFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. TEST THE MODEL ON 1 LINE**\n",
        "\n"
      ],
      "metadata": {
        "id": "-Cmr9V0NMNPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in dataset.take(1):\n",
        "    target_predictions= Encoder_model(input_batch)\n",
        "    print(target_predictions.shape,\"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "DxySdYqRMU21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we try the prediction batch."
      ],
      "metadata": {
        "id": "IN4pDtN9NToq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices= tf.random.categorical(target_predictions[0], num_samples=1) #target_predictions[0] is predicted probabilities (logits) for each possible next character in the text sequence. Typically, this is the output from a neural network's softmax layer.\n",
        "sampled_indices= tf.squeeze(sampled_indices, axis= -1).numpy() #gives 1D tensor converted to array"
      ],
      "metadata": {
        "id": "l4Uaq7bWNZPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", readable_text_from_indices(input_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", readable_text_from_indices(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "nupmp_PdOsrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. LOSS FUNCTION**\n",
        "\n",
        "Determine the loss from input and the predicted values."
      ],
      "metadata": {
        "id": "dXMl9DzgPRHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss =tf.losses.SparseCategoricalCrossentropy(from_logits= True)\n",
        "mean_loss_for_batch= loss(input_batch, target_predictions)\n",
        "print(\"Mean Loss: \", mean_loss_for_batch)"
      ],
      "metadata": {
        "id": "4wq8RTxhPaGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. TRAINING THE ENCODER**\n",
        "\n",
        "Configure epochs and checkpoints."
      ],
      "metadata": {
        "id": "9VJU81TJP75n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(mean_loss_for_batch).numpy()\n",
        "Encoder_model.compile(optimizer=\"adam\", loss=loss)\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Kpw43NniQEDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS= 10\n",
        "train_history= Encoder_model.fit(dataset, epochs= EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "_aHEPh9tQ8Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. BUILDING THE DECODER**\n",
        "\n",
        "\n",
        "*   Create a function to convert between characters and token ids.\n",
        "*   Create a mask to prevent [UNK] token generation.\n",
        "*   Create a function to take input tokens and previous hidden states.\n",
        "*   Convert input tokens to token IDs.\n",
        "*   Run the model with the input token IDs to get the predicted logits (scores) for the next token.\n",
        "*   Divide the logits by the temperature parameter to control randomness in token sampling.\n",
        "*   Apply the prediction mask to the logits to exclude certain tokens.\n",
        "*   Sample token IDs from the logits and convert them back to characters.\n",
        "*   Return the predicted characters and updated model states.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IbXx559LRejW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, Encoder_model, chars_from_ids, ids_to_chars, temperature= 1.0):\n",
        "    super().__init__()\n",
        "    self.temperature= temperature\n",
        "    self.model= Encoder_model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_to_chars = ids_to_chars\n",
        "    #create mask-----\n",
        "    mask_token= self.ids_to_chars([\"[UNK]\"])[:, None]\n",
        "    #at each bad index, place -inf. Also, match shape to vocabulary.\n",
        "    sparse_mask= tf.SparseTensor(values=[-float(\"inf\")] * len(mask_token), indices=mask_token, dense_shape=[len(ids_from_chars.get_vocabulary())] )\n",
        "    self.prediction_mask= tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "    def generate_one_step(self, inputs, states= None):\n",
        "    #give input to decoder some token ids\n",
        "    input_characters= tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "    input_ids= self.ids_to_chars(input_characters).to_tensor()\n",
        "\n",
        "    predicted_logits, states= self.Encoder_model(inputs=input_ids, states=states, return_state=True)\n",
        "\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits / self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask #prevent [UNK] from being generated\n",
        "\n",
        "    # get the token ids\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #get characters from ids\n",
        "    predicted_characters= self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_characters, states\n"
      ],
      "metadata": {
        "id": "iWk8WnCAedK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decoder_model= Decoder(Encoder_model, chars_from_ids, ids_to_chars)\n",
        "# give some prompt\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(2000):\n",
        "  next_char, states= Decoder_model.generate_one_step( next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "id": "Mgc8DQWYkE2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}